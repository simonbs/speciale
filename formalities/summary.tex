\section*{Summary}
\label{formalities:summary}

This thesis focuses on the design and implementation of a context based system for controlling a smart home environment utilizing gesture recognition on a smartwatch.

The system is designed to utilize available contextual information about the user and the environment he resides in to determine which devices he wishes to interact with and what change he intends to apply to it.
To limit the scope of the project the system only utilizes two sources of contextual information: Which gesture the user has performed and which room he is situated in.

The hardware requirements of the system is a smartwatch that supports Bluetooth and contains an accelerometer, a computer to act as a hub and one Estimote beacon per room in the smart home.
This project was implemented using a second generation Moto 360 smartwatch and a Raspberry Pi 3.

In order to recognize gestures performed by the user a mix between 1\textcent~\cite{herold20121} and \$3~\cite{threedollar} was created.
The gesture recognizer creates a timeseries of recorded accelerometer measurements by adding each succesive measurement to the previous one.
This trace is then resampled to contain exactly \emph{64} equidistant points and the centroid of the trace is determined.
The trace is then converted to a vector containing the distance from each point in the trace to the centroid which is stored as a \emph{template} which can be compared with other templates.

The second source of contextual information in this project is the location of the user which is obtained through use of Estimote beacons.
These beacons send out Bluetooth signals that are picked up by the smartwatch and based on the RSSI value the smartwatch determines which beacon is the closest and thus which room the user is in.

Users of the system can create configurations where combinations of a gesture, a room, a device and a supported action are selected.

In order to utilize the gesture and room information to determine an appropriate action to trigger, a Bayesian network was created that uses these configurations to determine which action is the most probable.

To evaluate how well the system performed when used by other people a small scale test was carried out where seven master students participated.
The participants were asked to train four unique gestures ten times each, resultin in a total of 40 templates.
The gesture shapes were predetermined by us and the participants were instructed how to perform them but were given no opportunity to practice them.
They were then asked to perform these gestures in different rooms simulated by turning the different beacons on and off.

The result was that the correct action was triggered 44\% of the time and a flaw in our Bayesian network was discovered which led us to search for an alternative model.
Of the alternative models considered, influence diagrams seemed the most promising.