\section{Prototype}
\label{sec:implementation:prototype}

This section presents the prototype of the Android Wear application as well as the \emph{binding}, \ie~an addon providing functionality specific to a problem domain, for openHAB developed during the project.

\subsection{Android Wear}
\label{sec:implementation:prototype:android-wear}

\Cref{fig:implementation:prototype:navigation-diagram} shows a navigation diagram for the user interface of the smartwatch application. The letters in the top right corner of the nodes in the diagram references the screenshots shown in \Cref{fig:implementation:prototype:screenshots}. The diagram shows that the application can be considered divided into the following two areas.

\begin{description}
\item[Primary use] We refer to the parts of the application which the user uses the most as the \emph{primary use}. This involves the recognition of gestures as well as the picker presented when an action to trigger could not be determined after context recognition.
\item[Configuration] The user will need to configure the application, \ie~train gestures and create gesture configurations. We refer to this part of the application as \emph{configuration}.
\end{description}

When opening the application, the user is presented with a screen to perform gesture recognition. This enables the user to start context recognition fast, whereas he needs to dive deeper into the navigation in order to train gestures and create gesture configurations, \ie~features that he will not use as often as the context recognition. For a list of the supported features in the application, refer to \Cref{sec:implementation:status}.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.7\textwidth]{images/wear-navigation-diagram}
    \caption{Navigation diagram for the user interface of the Android Wear application. Letters in the top right corner of each node refers to the screenshots in figure \Cref{fig:implementation:prototype:screenshots}.}
    \label{fig:implementation:prototype:navigation-diagram}
\end{figure}

\begin{figure}[!htb]%
    \centering
    \subbottom[Screen on which the user can start gesture recognition by touching the screen.]{\label{fig:implementation:prototype:screenshots:1}
        \frame{\includegraphics[width=0.3\textwidth]{images/wear-screenshot-touch-recognize}}
    }
    \subbottom[Screen on which the user can stop gesture recognition by touching the screen. The orange color indicates that recognition has been started.]{\label{fig:implementation:prototype:screenshots:2}
        \frame{\includegraphics[width=0.3\textwidth]{images/wear-screenshot-recognizing}}
    }
    \subbottom[Settings screen from which the user can set his virtual, train a gesture and configure a new action (create a gesture configuration). When a virtual position is set, the setting can be selected again to unet the virtual position.]{\label{fig:implementation:prototype:screenshots:3}
        \frame{\includegraphics[width=0.3\textwidth]{images/wear-screenshot-settings}}
    }
    \subbottom[Screen shown when the user starts training a gesture. Tapping the ``Gesture name'' button will present the user with a speech recognition interface from which he pronounce the name of the gesture to configure it.]{\label{fig:implementation:prototype:screenshots:4}
        \frame{\includegraphics[width=0.3\textwidth]{images/wear-screenshot-name-gesture}}
    }
    \subbottom[Screen shown when training a gesture and the gesture recognizer has not been started. We see that two gesture samples, \ie~gesture templates, have been trained.]{\label{fig:implementation:prototype:screenshots:5}
        \frame{\includegraphics[width=0.3\textwidth]{images/wear-screenshot-touch-train}}
    }
    \subbottom[Screen shown when training a gesture and the gesture recognizer has been started, \ie~the user is performing a gesture.]{\label{fig:implementation:prototype:screenshots:6}
        \frame{\includegraphics[width=0.3\textwidth]{images/wear-screenshot-training}}
    }
    \subbottom[Gesture picker showing the gestures the user has trained. The gesture picker is used when creating a gesture configuration.]{\label{fig:implementation:prototype:screenshots:7}
        \frame{\includegraphics[width=0.3\textwidth]{images/wear-screenshot-gestures}}
    }
    \subbottom[Room picker shown when the user creates a gesture configuration. The picker is also shown when setting a virtual position.]{\label{fig:implementation:prototype:screenshots:8}
        \frame{\includegraphics[width=0.3\textwidth]{images/wear-screenshot-rooms}}
    }
    \subbottom[Action picker shown when creating a gesture configuration. A similar picker is shown when the context engine suggests multiple actions.]{\label{fig:implementation:prototype:screenshots:9}
        \frame{\includegraphics[width=0.3\textwidth]{images/wear-screenshot-actions}}
    }
    \caption{Screenshots of the prototype developed for the Android Wear smartwatch.}
    \label{fig:implementation:prototype:screenshots}
\end{figure}

\subsection{openHAB}
\label{sec:implementation:prototype:openhab}

We developed a \emph{binding} for openHAB. A \emph{binding} is an addon for openHAB which relates to a specific problem domain. For example, there is a binding for communcating with Philips Hue lights and a binding for controlling a Sonos media centre. We have created a binding for configuring the rooms and beacons in a users home.

We chose to place the configuration of the rooms and beacons in openHAB, as this is a one time configuration which can be shared amongst the users in a home where as the creation of gesture configurations is place on the watch as this can be independent for each inhabitant of a home. The rooms and beacons are synchronized to the smartwatch when the application is loaded. The watch use the models when positioning the user.

A room has a name and when it is created, openHAB assigns it a UID. When creating a beacon, the user must enter the UID assigned by openHAB in order to specify which room the beacon is placed in. Furthermore the user should enter the Eddystone namespace and instance, both described in \Cref{sec:design:ble-positioning}. This identifies the specific beacon. When the smartwatch registers a beacon, it can determine which room the user is in by the beacons reference to a room.

In order to control Spotify running on a desktop machine, we created the Spotify Controls API. We configured openHAB to communicate with the API using the default Exec binding, a binding which executes a shell command when some event occurs in openHAB.

% \begin{figure}[!htb]%
%     \centering
%     \subbottom[Screen on which to choose the binding to configure.]{\label{fig:implementation:prototype:openhab:bindings}
%         \frame{\includegraphics[width=0.5\textwidth]{images/openhab-addon-bindings}}
%     }
%     \subbottom[Screen on which the \emph{thing} to create is chosen. In openHAB physical devices are referred to as \emph{things}.]{\label{fig:implementation:prototype:openhab:things}
%         \frame{\includegraphics[width=0.45\textwidth]{images/openhab-addon-things}}
%     }
%     \subbottom[Screen on which a new room can be created. The room should have a name, \eg~``Kitchen'' or ``Living room''.]{\label{fig:implementation:prototype:openhab:room}
%         \frame{\includegraphics[width=0.45\textwidth]{images/openhab-addon-room}}
%     }
%     \subbottom[Screen on which a new beacon can be created. The UID of the room the beacon is placed in must be specified. UIDs are created by openHAB when a new room is created. The namespace and instance of the Eddystone beacon must also be specified. ]{\label{fig:implementation:prototype:openhab:beacon}
%         \frame{\includegraphics[width=0.45\textwidth]{images/openhab-addon-beacon}}
%     }
%     \subbottom[Screen showing the list of all configured things in openHAB. The screenshot shows two rooms and two beacons configured as well as some Philips Hue light bulbs.]{\label{fig:implementation:prototype:openhab:list}
%         \frame{\includegraphics[width=0.45\textwidth]{images/openhab-addon-list}}
%     }
%     \caption{Screenshots of the binding developed for openHAB.}
%     \label{fig:implementation:prototype:openhab:screenshots}
% \end{figure}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../master"
%%% End:
