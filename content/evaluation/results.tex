\section{User Tests}
\label{sec:evaluation:user-tests}

This section will describe the experimental procedures carried out during the project to determine how well it performed when used by
other people and how well it performed comparing to the requirements specified in \cref{sec:requirements-specification}.

\subsection{Setup}
\label{sec:evaluation:user-tests-setup}

The setup of our user test consisted of a Macbook Pro running OpenHAB and Spotify, two Philips Hue lamps and two Estimote beacons.
Seven people were asked to train four unique gestures, creating ten templates per gesture resulting in a total of 40 gesture templates
per person.
The gestures used were:

\begin{itemize}
  \item Circle
  \item Swipe left to right
  \item V
  \item Zorro
\end{itemize}

The setup was limited to a subset of the smart devices and gestures presented in the scenario \cref{sec:analysis:scenarios}, because we did not have the necessary hardware and space available to perform a test of the entire scenario as well as to keep the invested time for each participant to a minimum, so it is more likely that they schedule time to participate in a test.

The gesture templates stored in the database were removed before each test so each participant was only using his own templates and not those created by others.

The participants were instructed how to perform the gestures but were allowed to scale them to their personal preference, eg.
either create large circles or small circles.

Once the gesture templates had been created the participants were asked to complete seven tasks, each of which required them to perform a given gesture five times.
Tests took place in a single room but to simulate the user moving between two different rooms, only one Estimote beacon would be turned on at a time.
A list of the gestures and locations is shown in in \cref{table:user-test-tasks}, along with the OpenHAB actions they were supposed to trigger.

\begin{table}[]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Action}                  & \textbf{Gesture}             & \textbf{Location}                       \\ \hline
Shelves\_Lamp\_Toggle   & V                   & Home Office                    \\ \hline
Spotify\_PlayPause      & Circle              & Home Office                    \\ \hline
Spotify\_Next           & Swipe Left to Right & Home Office                    \\ \hline
Architect\_Lamp\_Toggle & V                   & Living Room                    \\ \hline
TV\_Lamp\_Toggle        & Zorro               & Living Room                    \\ \hline
Spotify\_Next           & Swipe Left to Right & Home Office (Virtual Position) \\ \hline
Shelves\_Lamp\_Toggle   & V                   & Home Office                    \\ \hline
\end{tabular}
\caption{The actions, gestures and locations that were used during the user tests.}
\label{table:user-test-tasks}
\end{table}

\subsection{Results}
\label{sec:evaluation:user-tests-results}

For each participant and task, the correctness ratio of the actions triggered was calculated as the \emph{number of times the intended action was triggered} divided by the \emph{number of attempts} and can be found in \cref{fig:user-test-action-correctness}.
In the same manner, the correctness ratio for gestures and locations can be found in \cref{fig:user-test-gesture-correctness,fig:user-test-location-correctness}.
For each participant, the average correctness rate for all actions, gestures, and locations can be found in \cref{table:user-task-averages}.
\Cref{fig:participant-average} shows the combined averages of all participents separated per task.
This shows that the locations is correctly identified in the majority of the cases with an average correctness ratio of 0.827020202, which is above the requirement specified in \cref{sec:requirements-specification}.
The average correctness ratio of actions is well below 80\% with a combined average of 0.4361381674.
Triggering the correct action less than half of the time is not a satisfactory result and is thusly something that needs to be looked into.
We suspect that the primary source of the inaccuracy comes from the low average correctness ratios of the gestures.
\cref{fig:participant-average} shows that the gesture correctness ratios are generally higher than those for actions yet never surpass the 80\% requirement with an average value of 0.5567550505.
However differences between the average gesture correctness ratios of the participants are noticable.
The most noticable example is comparing \cref{fig:participant-1,fig:participant-2}.
Something to note here is that both of these participants, as well as participant 1 experienced some technical difficulties which prevented them from completing the task.

\begin{figure}[!htb]
\begin{longtable}{p{0.45\textwidth} p{0.45\textwidth}}
  \centering
  \begin{tikzpicture}
  \begin{axis}[
    ybar,
    bar width=2pt,
    xticklabels from table={data/ActionCorrectnessTransposed.csv}{Action},
    xtick=data,
    x tick label style={
    rotate=45,
    anchor=east,
    },
    width=0.40\textwidth,
    ymin = 0,
    ymax = 1,
    ylabel = Correctness Ratio,
    legend style={
    at={(0,0)},
    anchor=west, at={(axis description cs:1,0.5)}}]
    \addplot table[x=Row, y=2] {data/GestureCorrectness.csv};
    \addplot table[x=Row, y=2] {data/ActionCorrectnessTransposed.csv};
    \addplot table[x=Row, y=2] {data/LocationCorrectness.csv};
    \legend{Gesture, Action, Location}
  \end{axis}
  \end{tikzpicture}
  \figcaption{Participant 2.}
  \label{fig:participant-1}
&
  \begin{tikzpicture}
  \begin{axis}[
    ybar,
    bar width=2pt,
    xticklabels from table={data/ActionCorrectnessTransposed.csv}{Action},
    xtick=data,
    x tick label style={
    rotate=45,
    anchor=east,
    },
    width=0.40\textwidth,
    ymin = 0,
    ymax = 1]
    \addplot table[x=Row, y=6] {data/GestureCorrectness.csv};
    \addplot table[x=Row, y=6] {data/ActionCorrectnessTransposed.csv};
    \addplot table[x=Row, y=6] {data/LocationCorrectness.csv};
  \end{axis}
  \end{tikzpicture}
  \figcaption{Participant 6.}
  \label{fig:participant-2}
\end{longtable}
\caption{Average correctness ratio for gestures, actions and locations for the participants who performed the best and worst respectively.}
\end{figure}

\begin{figure}[!htb]
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar,
    bar width=2pt,
    xticklabels from table={data/ActionCorrectnessTransposed.csv}{Action},
    xtick=data,
    x tick label style={
    rotate=45,
    anchor=east,
    },
    width=0.42\textwidth,
    ymin = 0,
    ymax = 1,
    ylabel = Correctness Ratio,
    legend style={
    at={(0,0)},
    anchor=north west, at={(axis description cs:1,0)}}]
    \addplot table[x=Row, y=Average] {data/GestureCorrectness.csv};
    \addplot table[x=Row, y=Average] {data/ActionCorrectnessTransposed.csv};
    \addplot table[x=Row, y=Average] {data/LocationCorrectness.csv};
    \legend{Gesture, Action, Location}
\end{axis}
\end{tikzpicture}
\caption{All participants combined.
Bars in order from left to right:
\emph{Gesture correctness ratio}, \emph{Action correctness ratio}, \emph{Location correctness ratio}}
\end{figure}

\subsection{Conclusion}
\label{sec:evaluation:user-tests-conclusion}

Comparing the results with the requirements specified in \cref{sec:requirements-specification}, only the positioning part succeeded.
The low successrate of actions is likely caused by the low successrate of the gesture but may also be due to the way we have modeled the Bayesian network.
Though the requirements were not fulfilled, the average correctness ratios seem promising and with a bit more work done on improving the gesture recognition, and a perhaps the Bayesian network this project seems feasible.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../master"
%%% End:
